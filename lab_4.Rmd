---
title: "Лабораторная_4"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
  word_document: default
---


### Линейные регрессионные модели   

В данной задаче выполняются следующие пункты:     

* оценка параметров модели линейной регрессии   
* построение модели со взаимосдействием переменных   
* визуализация и тестирование остатков модели линейной регрессии    

*Модели*: множественная линейная регрессия, kNN.   
*Данные*: статистика стоимости жилья в пригороде Бостона.  

Цель: исследовать набор данных `Boston` с помощью линейной регрессионной модели. Задействовав все возможные регрессоры, сделать вывод о пригодности модели для прогноза. Сравнить с методом k ближайших соседей по MSE на тестовой выборке.    


```{r Данные и пакеты, warning = F, message = F}
# загрузка пакетов
library('GGally')       # графики совместного разброса переменных
library('lmtest')       # тесты остатков регрессионных моделей
library('FNN')          # алгоритм kNN
library('mlbench')
library('MASS')
# константы
my.seed <- 12345
train.percent <- 0.85
data(Boston)            # открываем данные
?Boston
Boston <- Boston[,-c(2,5,6,8,9,10,11,12,13,14)]
Boston$chas <- as.factor(Boston$chas)

# обучающая выборка
set.seed(my.seed)
inTrain <- sample(seq_along(Boston$crim), 
                  nrow(Boston) * train.percent)
df.train <- Boston[inTrain, c(colnames(Boston)[-1], colnames(Boston)[1])]
df.test <- Boston[-inTrain, -1]
```

## Описание переменных

- `crim` – уровень преступности на душу населения по городам;  
- `indus` – доля акров некоммерческого бизнеса в городе;
- `age` – доля занятых собственниками, построенных до 1940 года. 
- `chas` - фиктивная переменная реки Чарльз (1, если тракт ограничивает реку; 0 в противном случае).
Размерность обучающей выборки: $n = `r dim(df.train)[1]`$ строк, p = 3 объясняющих переменных. Зависимая переменная -- `crim`.  


```{r Описание данных-02, message = F, warning = F}
# описательные статистики по переменным
summary(df.train)
# совместный график разброса переменных
ggp <- ggpairs(df.train)
print(ggp, progress = F)
# цвета по фактору chas
ggp <- ggpairs(df.train, mapping = ggplot2::aes(color = chas))
print(ggp, progress = F)

```

Коробчатые диаграммы на пересечении chas & indus и chas & age показывают, что если в пригороде тракт не ограничивает реку (chas = 0), то доля акров некоммерческого бизнеса в городе и доля занятых собственниками, построенных до 1940 года, будут выше. Однако и уровень преступности в такой местности будет значительно превосходить, чем в местности с установленным трактом. 

## Модели  

```{r , warning = F, error = F}
model <- lm(crim ~ age + indus + chas,
              data = df.train)
summary(model)

```

В модели все объясняющие переменные ялвяются значимыми. Проверим её остатки. 

# Проверка остатков  

```{r , warning = F, error = F}
# тест Бройша-Пагана
bptest(model)
# статистика Дарбина-Уотсона
dwtest(model)
# графики остатков
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))
plot(model, 1)
plot(model, 4)
plot(model, 5)
par(mfrow = c(1, 1))
```

Судя по графику слева, остатки не случайны, и их дисперсия непостоянна. В модели есть три влиятельных наблюдения: 405, 411, 415 -- которые, однако, не выходят за пределы доверительных границ на третьем графике. Графики остатков заставляют усомниться в том, что остатки удовлетворяют условиям Гаусса-Маркова.      

# Сравнение с kNN

```{r }
y.fact <- Boston[-inTrain, 1]
y.model.lm <- predict(model, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)
# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(apply(df.train, 2, as.numeric))
df.test.num <- as.data.frame(apply(df.test, 2, as.numeric))
for (i in 2:50){
  model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% 'crim')], 
                       y = df.train.num[, 'crim'], 
                       test = df.test.num, k = i)
  y.model.knn <- model.knn$pred
  if (i == 2){
    MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
  } else {
    MSE.knn <- c(MSE.knn, 
                 sum((y.model.knn - y.fact)^2) / length(y.model.knn))
  }
}
# график
par(mar = c(4.5, 4.5, 1, 1))
# ошибки kNN
plot(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2,
     xlab = 'значение k', ylab = 'MSE на тестовой выборке')
# ошибка регрессии
lines(2:50,  MSE.knn, type = 'b', col = 'darkgreen')
legend('bottomright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))
```

```{r, include = F}
frml.to.text.01 <- paste0('$\\frac {\\sqrt{MSE_{TEST}}}{\\bar{y}_{TEST}} = ',
                          round(sqrt(MSE.lm) / mean(y.fact) * 100, 1),
                          '\\%$')
```


Как можно видеть по графику, ошибка регрессии на тестовой выборке при всех значениях k больше, чем ошибка метода k ближайших соседей. С увеличением количества соседей точность kNN практически не изменяется,  что говорит о том, что данная регрессионная модель пригодна для прогнозирования. 
